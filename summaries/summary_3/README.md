## Summary of the fifth chapter of the book "Designing Machine Learning Systems" by Chip Huyen - Feature Engineering

The rapid spread of Machine Learning models across a variety of applications has underscored the importance of feature engineering in enhancing the performance and effectiveness of these models. In this regard, Chapter Five of the book "Designing Machine Learning Systems" by Chip Huyen addresses feature engineering, a crucial process for building effective Machine Learning systems. The author emphasizes the importance of high-quality features and the need to understand the problem domain to create relevant and informative features.

Despite the significant advances in deep learning, which has enabled the automated learning of many features, the chapter notes that there are still many features that need to be manually extracted. For example, in building a sentiment analysis classifier, techniques such as lemmatization, punctuation removal, and stopword removal are applied. Instead of focusing on these techniques, the author suggests simply splitting the raw text into words, i.e., performing tokenization. Regarding images, the author mentions that one can directly add the raw image to the model, rather than manually extracting features. However, often it is necessary to have data beyond images and text, such as the frequency of behavior of a post, links, etc.

**Feature engineering operations** are fundamental to enhancing the performance of models in Machine Learning projects, ranging from handling missing values to scaling data, through discretization and encoding of categorical features. Handling missing values is particularly complex, with three main types identified: missing not at random (MNAR), where the absence of data is directly related to the value itself; missing at random (MAR), influenced by other observed variables; and missing completely at random (MCAR), with no discernible pattern. Thus, exclusion and imputation emerge as the main treatment methods, each with its advantages and challenges, highlighting the complexity of feature engineering in ensuring data integrity.

On the other hand, **scaling** is essential for harmonizing ranges of variable values, preventing large discrepancies from negatively influencing ML models. Techniques such as normalization (adjusting data to a range [0,1] or [-1,1]) and standardization (adjusting data to have zero mean and unit variance) are crucial, especially when dealing with features that have asymmetric distributions. Furthermore, logarithmic transformation can be applied to mitigate asymmetries in data. However, these operations require careful attention to avoid data leakage and ensure that the global statistics used during training are relevant during inference, highlighting the need for frequent reassessments and adjustments to models.

Another aspect highlighted by the author is **discretization**, the process of converting continuous features into discrete ones, also known as quantization or categorization. This technique aims to simplify the learning process of models, especially with limited data, by grouping similar values together, allowing the model to treat them equivalently. Although typically applied to continuous features, discretization can also be useful for discrete data, such as in the case of age categories. However, this approach can introduce discontinuities, which can be mitigated by defining more refined discretization intervals. The selection of these intervals often requires specific domain knowledge. Despite its potential advantages, the technique is rarely employed in practice.

Furthermore, Chip Huyen discusses the **encoding of categorical features**, a crucial process for converting categorical variables into numerical ones, which has proven to be a challenging task, especially in production environments where categories such as product brands or user accounts may change frequently. This problem is illustrated with the example of a recommendation system at Amazon, which needs to handle the constant change of products and categories. This can lead to model failures if proper maintenance is not performed. Thus, a possible solution discussed is the hashing trick, where each category is mapped to a unique index within a fixed hash space, mitigating problems of new categories and index collisions. While sometimes seen as a makeshift in academia, the technique is valued in the industry for its effectiveness and is adopted in continuous learning and on major platforms such as scikit-learn and TensorFlow.

Additionally, the discussion covers techniques of **feature crossing** and **positional shuffling**. The former involves combining two or more features to model nonlinear relationships between them, such as the interaction between age and income, while the latter aims to enhance model generalization by preventing it from memorizing the order of training data.

The chapter also addresses **data leakage**, a common problem in Machine Learning projects where information from the test set is accidentally included in the training set, leading to overestimated and ineffective models. The text also cites a case of leakage in a Kaggle competition, where participants exploited leaked test data to gain an advantage. Additionally, common causes of leakage are explored, such as incorrect splitting of temporal data, improper scaling, inadequate filling of missing data, incorrect handling of duplicates, and careless splitting of data that should be grouped.

Finally, several strategies to **avoid data leakage** are presented, emphasizing the need for constant monitoring throughout the project lifecycle, from data generation to processing. The author highlights the importance of maintaining data integrity, alerting to the dangers of interacting with the test set in ways that could induce leakage. The issue of balancing the addition of features is also addressed, which although it can improve model performance, also increases the risk of overfitting and data leakage, as well as raising memory and latency requirements.

Thus, best practices in feature engineering to ensure the success of a Machine Learning model include dividing the data into train, validation, and test sets based on time, not randomly, and performing oversampling after this division. Moreover, it is essential to normalize and scale the data after splitting to avoid information leakage, using only training data for these adjustments. A deep understanding of how data is generated, collected, and processed proves essential, as is valuable support from domain experts and rigorous monitoring of data origins. These practices contribute to building effective and robust models, capable of better generalization and minimizing risks associated with information leakage.